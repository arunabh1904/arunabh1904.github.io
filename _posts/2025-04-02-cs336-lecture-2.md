---
layout: post
title: "CS336 Lecture 2 Notes - Tensor Fundamentals"
date: 2025-04-02 00:00:00 -0400
categories: ["Revision Notes"]
---

## Lecture 2 Notes – CS336 (Percy Liang): Tensor Fundamentals and Computation Efficiency

This is the second installment in my revision notes for Percy Liang's CS336 course. Below I cover
Tensor fundamentals, computational efficiency, and key PyTorch concepts.

---
```
  ____  ____  ___
 / ___||  _ \|_ _|
| |    | | | || |
| |___ | |_| || |
 \____||____/|___|
```

### 1. Float Types: Which to Use and Why?

Different float types balance precision, memory usage, and speed. Here's a quick guide:

```python
import torch

dtypes = {
    "float64 (double)": torch.float64,  # high precision, mostly CPU
    "float32": torch.float32,           # default for training
    "bfloat16": torch.bfloat16,         # efficient on TPUs and modern GPUs
    "float16": torch.float16,           # inference or older GPUs
    "float8_e4m3": torch.float8_e4m3fn, # cutting-edge GPUs for inference
}

for name, dt in dtypes.items():
    x = torch.tensor([1.0, 2.0], dtype=dt, device="cuda" if torch.cuda.is_available() else "cpu")
    print(f"{name:<15} size={x.element_size()} bytes, x={x}")
```

---

### 2. Tensors and PyTorch Operations

A tensor is an n-dimensional array with metadata (dtype, device, layout, requires_grad). This
metadata drives PyTorch operations.

```python
t = torch.randn(2, 3, 4, device="cuda", requires_grad=True)
print(f"Tensor shape {t.shape}, dtype={t.dtype}, device={t.device}, contiguous={t.is_contiguous()}")

# Basic PyTorch operations
y = torch.relu(t)
z = torch.einsum("bcd, d -> b c", t, torch.randn(4, device=t.device))
loss = z.mean()
loss.backward()
print("Gradients shape:", t.grad.shape)
```

---

### 3. Views vs Copies in Tensors

When do operations copy tensors?

- **No Copy (Views):** stride-based operations, e.g., `a[:, ::2]`
- **Forced Copy:** `.clone()`, `.contiguous()` on non-contiguous tensors, or overlapping memory ops

```python
a = torch.arange(8).reshape(2, 4)
b = a[:, ::2]        # view, no copy
c = a.clone()        # explicit copy
d = a.contiguous()   # copy if non-contiguous

print(a.storage().data_ptr() == b.storage().data_ptr(),  # True (no copy)
      a.storage().data_ptr() == c.storage().data_ptr(),  # False (copy)
      a.storage().data_ptr() == d.storage().data_ptr())  # False (copy)
```

---

### 4. Einops: Readable Tensor Manipulation

Stop guessing tensor dimensions. Use Einops for clarity:

```python
from einops import rearrange, reduce

img = torch.randn(1, 3, 224, 224)  # Batch, Channel, Height, Width
patches = rearrange(img, "b c (h ph) (w pw) -> b (h w) (ph pw c)", ph=16, pw=16)
print("ViT-style patches shape:", patches.shape)

mean_per_patch = reduce(patches, "b n d -> b n", "mean")
print("Mean per patch shape:", mean_per_patch.shape)
```

---

### 5. FLOPs: Understanding Matrix Multiplication

Matrix multiplication FLOPs (floating point operations) formula:

\[
\text{FLOPs} = 2 \times m \times k \times n
\]

```python
def matmul_flops(m, k, n):
    return 2 * m * k * n

M, K, N = 1024, 1024, 4096
flops = matmul_flops(M, K, N)
print(f"Matmul ({M}x{K})x({K}x{N}) ≈ {flops/1e9:.2f} GFLOPs")
```

---

### 6. MFU: Measuring Computational Efficiency

Model FLOPs Utilization (MFU) measures actual performance relative to hardware peak.

```python
props = torch.cuda.get_device_properties(0)
peak_flops = (
    2 * props.multi_processor_count * props.clock_rate * 1e6 *
    props.max_threads_per_multi_processor
)

A, B = torch.randn(M, K, device="cuda"), torch.randn(K, N, device="cuda")
start, end = torch.cuda.Event(True), torch.cuda.Event(True)
start.record(); C = A @ B; end.record()
torch.cuda.synchronize()

ms = start.elapsed_time(end)
actual_flops = flops / (ms / 1e3)
mfu = actual_flops / peak_flops

print(f"MFU ≈ {mfu:.2%}")
```

Real-world MFU rarely hits 100% due to memory transfers and pipeline stalls.

---

### 7. Backward-Pass FLOPs Calculation

Backward pass computations typically cost about three times the forward pass for matrix
multiplication:

```python
def matmul_bwd_flops(m, k, n):
    return 6 * m * k * n

print(f"Backward pass FLOPs ≈ {matmul_bwd_flops(M, K, N)/1e9:.2f} GFLOPs")
```

---

### 8. Xavier (Glorot) Initialization

Xavier initialization maintains activation variance to stabilize learning.

```python
import math

def xavier_uniform_(tensor):
    fan_in, fan_out = tensor.size(1), tensor.size(0)
    limit = math.sqrt(6.0 / (fan_in + fan_out))
    return torch.nn.init.uniform_(tensor, -limit, limit)

linear = torch.nn.Linear(784, 256)
xavier_uniform_(linear.weight)
print("Xavier initialized weights std:", linear.weight.std())
```

---

Stay tuned for more notes!
