---
layout: post
title: "CS336 Lecture 2 Notes - Tensor Fundamentals"
date: 2025-04-02 00:00:00 -0400
categories: ["Revision Notes"]
---

## Lecture 2 Notes – CS336 (Percy Liang): Tensor Fundamentals and Computation Efficiency

This is the second installment in my revision notes for Percy Liang's CS336 course. Below I cover
Tensor fundamentals, computational efficiency, and key PyTorch concepts.

### Lecture Outline

1. Floating-Point DTypes
2. Tensor Anatomy & Autograd
3. Views vs Copies
4. Einops for Readable Reshaping
5. FLOPs Accounting
6. Model FLOP Utilisation (MFU)
7. Xavier (Glorot) Initialisation

---
```
  ____  ____  ___
 / ___||  _ \|_ _|
| |    | | | || |
| |___ | |_| || |
 \____||____/|___|
```

### 1. Float Types: Which to Use and Why?

Different float types balance precision, memory usage, and speed:

- **float64** provides research-grade precision but is memory heavy.
- **float32** is the default training dtype with wide library support.
- **bfloat16** keeps the float32 exponent so models usually train well while halving memory.
- **float16** also halves memory but can underflow or overflow in deep nets.
- **float8_e4m3** is experimental and mainly used in specialised inference kernels.

```python
import torch

dtypes = {
    "float64 (double)": torch.float64,  # high precision, mostly CPU
    "float32": torch.float32,           # default for training
    "bfloat16": torch.bfloat16,         # efficient on TPUs and modern GPUs
    "float16": torch.float16,           # inference or older GPUs
    "float8_e4m3": torch.float8_e4m3fn, # cutting-edge GPUs for inference
}

for name, dt in dtypes.items():
    x = torch.tensor([1.0, 2.0], dtype=dt)
    print(f"{name:<15} | {x.element_size()} bytes")
```

---

### 2. Tensor Anatomy & Autograd

A tensor is an n-dimensional array with metadata (dtype, device, layout, requires_grad). This
metadata drives PyTorch operations.

```python
t = torch.randn(2, 3, 4, device="cuda", requires_grad=True)
print(f"Tensor shape {t.shape}, dtype={t.dtype}, device={t.device}, contiguous={t.is_contiguous()}")

# Basic PyTorch operations
y = torch.relu(t)
z = torch.einsum("bcd, d -> b c", t, torch.randn(4, device=t.device))
loss = z.mean()
loss.backward()
print("Gradients shape:", t.grad.shape)
```

---

### 3. Views vs Copies

When do operations copy tensors?

- **No Copy (Views):** stride-based operations, e.g., `a[:, ::2]`
- **Forced Copy:** `.clone()`, `.contiguous()` on non-contiguous tensors, or overlapping memory ops

```python
a = torch.arange(8).reshape(2, 4)
b = a[:, ::2]        # view, no copy
c = a.clone()        # explicit copy
d = a.contiguous()   # copy if non-contiguous

print(a.storage().data_ptr() == b.storage().data_ptr(),  # True (no copy)
      a.storage().data_ptr() == c.storage().data_ptr(),  # False (copy)
      a.storage().data_ptr() == d.storage().data_ptr())  # False (copy)
```

---

### 4. Einops for Readable Reshaping

Stop guessing tensor dimensions. Use Einops for clarity:

```python
from einops import rearrange, reduce

img = torch.randn(1, 3, 224, 224)  # Batch, Channel, Height, Width
patches = rearrange(img, "b c (h ph) (w pw) -> b (h w) (ph pw c)", ph=16, pw=16)
print("ViT-style patches shape:", patches.shape)

mean_per_patch = reduce(patches, "b n d -> b n", "mean")
print("Mean per patch shape:", mean_per_patch.shape)
```

---

### 5. FLOPs Accounting

Matrix multiplication FLOPs (floating point operations) formula:

\[
\text{FLOPs} = 2 \times m \times k \times n
\]

```python
def matmul_flops(m, k, n):
    return 2 * m * k * n  # forward

def matmul_bwd_flops(m, k, n):
    return 6 * m * k * n  # backward ≈ 3× forward

M, K, N = 1024, 1024, 4096
print(f"Forward:  {(matmul_flops(M, K, N)/1e9):6.2f} GFLOPs")
print(f"Backward: {(matmul_bwd_flops(M, K, N)/1e9):6.2f} GFLOPs")
```

---

### 6. MFU (Model FLOP Utilisation)

Model FLOPs Utilization (MFU) measures actual performance relative to hardware peak.

```python
import time, torch

def peak_flops(device):
    p = torch.cuda.get_device_properties(device)
    return 2 * p.multi_processor_count * p.clock_rate * 1e6 * \
           p.max_threads_per_multi_processor  # FP32 MACs / s

def mfu_demo(m=1024, k=1024, n=4096):
    if not torch.cuda.is_available():
        print("CUDA not available – skipping MFU.")
        return
    device = torch.device("cuda")
    A, B = torch.randn(m, k, device=device), torch.randn(k, n, device=device)
    flops = matmul_flops(m, k, n)
    peak = peak_flops(device)

    torch.cuda.synchronize()
    t0 = time.perf_counter()
    _ = A @ B
    torch.cuda.synchronize()
    dt = time.perf_counter() - t0

    achieved = flops / dt
    print(f"MFU = {achieved/peak:5.2%}  ({achieved/1e12:5.2f} TFLOPs)")

mfu_demo()
```

Real-world MFU rarely hits 100% due to memory transfers and pipeline stalls.


### 7. Xavier (Glorot) Initialisation

Xavier initialization maintains activation variance to stabilize learning.

```python
import math

def xavier_uniform_(tensor):
    fan_in, fan_out = tensor.size(1), tensor.size(0)
    limit = math.sqrt(6.0 / (fan_in + fan_out))
    return torch.nn.init.uniform_(tensor, -limit, limit)

linear = torch.nn.Linear(784, 256)
xavier_uniform_(linear.weight)
print("Xavier initialized weights std:", linear.weight.std())
```

---

Stay tuned for more notes!
